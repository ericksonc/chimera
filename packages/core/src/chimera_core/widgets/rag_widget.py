"""RAGWidget - Retrieval-Augmented Generation for document context.

This widget provides semantic search over documents using embeddings:
1. Loads documents from specified paths (like ContextDocsWidget)
2. Generates embeddings on first use (lazy loading, per-file hashing)
3. Provides search_documents tool for the agent to query

Key features:
- ChromaDB for vector storage (global collection per base_path)
- Voyage AI (voyage-3-large) for embeddings
- Context-aware chunking (400-600 tokens)
- Per-file hash tracking (only changed files trigger re-embedding)
- Query-time filtering (widget config determines which files to search)
- Cross-widget reuse (shared knowledge base per project)
"""

# Import file handling utilities from ContextDocsWidget's module
# We'll reuse the same patterns
import fnmatch
import hashlib
import mimetypes
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Optional

import pathspec
from pydantic_ai import FunctionToolset

from chimera_core.widget import Widget
from chimera_core.widgets.rag import (
    ChunkParams,
    DocumentChunker,
    GlobalRAGRegistry,
    VectorStore,
    VoyageEmbeddingService,
)

if TYPE_CHECKING:
    from pydantic_graph.beta import StepContext

    from chimera_core.agent import Agent
    from chimera_core.protocols import ReadableThreadState
    from chimera_core.threadprotocol.blueprint import ComponentConfig


@dataclass
class RAGConfig:
    """Configuration for RAGWidget.

    Stored in BlueprintProtocol (Turn 0 config).
    """

    base_path: str  # Base directory for file loading
    whitelist_paths: list[str]  # Patterns to include
    blacklist_paths: list[str]  # Patterns to exclude
    chunk_min_tokens: int = 400  # Minimum tokens per chunk
    chunk_max_tokens: int = 600  # Maximum tokens per chunk
    top_k: int = 5  # Number of results to return for searches
    ambient_only: bool = (
        False  # If True, inject RAG results into ambient context instead of providing tool
    )
    include_neighboring_chunks: bool = (
        False  # If True, include chunks before/after matched chunks for context
    )


class RAGWidget(Widget[RAGConfig]):
    """Provides RAG-based document search via semantic embeddings.

    This widget operates in two modes:

    **Tool Mode (ambient_only=False, default)**:
    - Provides search_documents tool for explicit agent queries
    - Agent decides when to search and what to search for
    - Results returned via tool calls

    **Ambient Mode (ambient_only=True)**:
    - Automatically searches on every user input
    - Injects results into ambient context (ephemeral, not persisted)
    - No tool provided - agent sees results transparently
    - Query extracted from user prompt (last 1000 chars if >1000)

    **Context Expansion (include_neighboring_chunks=True)**:
    - Includes chunks before/after matched chunks for additional context
    - Format: [Context: chunk N-1] â†’ >> [Matched: chunk N] << â†’ [Context: chunk N+1]
    - Works in both tool and ambient modes
    - Handles edge cases (first/last chunks gracefully)

    Common features:
    1. Loads documents at initialization
    2. Generates embeddings on first use (lazy, per-file hashing)
    3. Global collection per base_path (cross-widget reuse)

    Example (Tool Mode):
        widget = RAGWidget(
            base_path="/Users/me/project",
            whitelist_paths=["docs/", "core/"],
            blacklist_paths=["*/archive/"]
        )
        agent.register_widget(widget)

    Example (Ambient Mode with Context):
        widget = RAGWidget(
            base_path="/Users/me/project",
            whitelist_paths=["docs/", "core/"],
            blacklist_paths=["*/archive/"],
            ambient_only=True,
            include_neighboring_chunks=True
        )
        agent.register_widget(widget)
    """

    # Component metadata
    # component_class_name auto-generated by PluginMeta as "core.widgets.rag_widget.RAGWidget"
    component_version = "1.0.0"

    def __init__(
        self,
        base_path: str,
        whitelist_paths: list[str],
        blacklist_paths: list[str] | None = None,
        chunk_min_tokens: int = 400,
        chunk_max_tokens: int = 600,
        top_k: int = 5,
        ambient_only: bool = False,
        include_neighboring_chunks: bool = False,
    ):
        """Initialize RAGWidget.

        Args:
            base_path: Base directory for file loading
            whitelist_paths: Patterns to include (e.g., "docs/", "core/**/*.py")
            blacklist_paths: Patterns to exclude (e.g., "*/archive/")
            chunk_min_tokens: Minimum tokens per chunk
            chunk_max_tokens: Maximum tokens per chunk
            top_k: Number of search results to return
            ambient_only: If True, inject RAG results into ambient context instead of providing tool
            include_neighboring_chunks: If True, include chunks before/after matched chunks for context
        """
        super().__init__()

        self.base_path = Path(base_path)
        self.whitelist_paths = whitelist_paths
        self.blacklist_paths = blacklist_paths or []
        self.chunk_min_tokens = chunk_min_tokens
        self.chunk_max_tokens = chunk_max_tokens
        self.top_k = top_k
        self.ambient_only = ambient_only
        self.include_neighboring_chunks = include_neighboring_chunks

        # Load documents (similar to ContextDocsWidget)
        self.documents: dict[str, str] = {}
        self.total_chars = 0

        # Load .gitignore if it exists
        self._gitignore_spec: Optional[pathspec.PathSpec] = None
        gitignore_path = self.base_path / ".gitignore"
        if gitignore_path.exists():
            try:
                with open(gitignore_path, "r") as f:
                    self._gitignore_spec = pathspec.PathSpec.from_lines("gitwildmatch", f)
            except Exception as e:
                print(f"âš ï¸ Could not load .gitignore: {e}")

        self._load_documents()

        # Initialize RAG components
        self.chunker = DocumentChunker(
            target_min_tokens=chunk_min_tokens, target_max_tokens=chunk_max_tokens
        )
        self.vector_store = VectorStore()
        # Embedding service is created lazily (needs async context)
        self._embedding_service: Optional[VoyageEmbeddingService] = None
        # Global registry for cross-collection deduplication
        self.global_registry = GlobalRAGRegistry()

        # Track which files are active for this widget instance (for query filtering)
        self.active_file_paths: list[str] = []
        # Track initialization status
        self._initialized: bool = False
        # Store last search results for ambient mode (ephemeral, per-turn)
        self._ambient_results: Optional[str] = None

    @classmethod
    def from_yaml(cls, yaml_path: Path | str, **overrides) -> "RAGWidget":
        """Load RAGWidget from YAML collection file.

        This factory method enables unified configuration across all RAG interfaces:
        - RAGWidget instances from shared YAML configs
        - cli/rag collections
        - ink_search tool collections
        - Blueprint references to YAML (instead of hardcoded paths)

        Args:
            yaml_path: Path to YAML file. Supports:
                - Absolute path: "/abs/path/to/custom.yaml"
                - Relative to cli/rag/collections/: "ink_source.yaml"
                - Collection name without extension: "ink_source"
            **overrides: Optional parameter overrides (e.g., top_k=10, ambient_only=True)

        Returns:
            RAGWidget instance configured from YAML

        Raises:
            FileNotFoundError: If YAML file not found
            ValueError: If YAML is invalid or missing required fields

        Example:
            # Load from collection name
            widget = RAGWidget.from_yaml("ink_source")

            # Load with overrides
            widget = RAGWidget.from_yaml("ink_all", ambient_only=True, top_k=10)

            # Load from absolute path
            widget = RAGWidget.from_yaml("/path/to/custom.yaml")
        """
        import yaml

        # Resolve path
        resolved_path = cls._resolve_yaml_path(yaml_path)

        if not resolved_path.exists():
            raise FileNotFoundError(
                f"RAG collection not found: {yaml_path}\nTried: {resolved_path}"
            )

        # Load YAML
        try:
            with open(resolved_path, "r") as f:
                config = yaml.safe_load(f)
        except Exception as e:
            raise ValueError(f"Failed to parse YAML {resolved_path}: {e}") from e

        # Validate config is a mapping
        if not isinstance(config, dict):
            raise ValueError(
                f"YAML {resolved_path} must be a mapping with collection fields, "
                f"got {type(config).__name__}"
            )

        # Validate required fields
        required = ["base_path", "whitelist_paths"]
        missing = [field for field in required if field not in config]
        if missing:
            raise ValueError(f"YAML {resolved_path} missing required fields: {missing}")

        # Extract config with defaults, apply overrides
        params = {
            "base_path": config["base_path"],
            "whitelist_paths": config["whitelist_paths"],
            "blacklist_paths": config.get("blacklist_paths", []),
            "chunk_min_tokens": config.get("chunk_min_tokens", 400),
            "chunk_max_tokens": config.get("chunk_max_tokens", 600),
            "top_k": config.get("top_k", 5),
            "ambient_only": config.get("ambient_only", False),
            "include_neighboring_chunks": config.get("include_neighboring_chunks", False),
        }
        params.update(overrides)

        return cls(**params)

    @staticmethod
    def _resolve_yaml_path(yaml_path: Path | str) -> Path:
        """Resolve YAML path with fallback logic.

        Resolution order:
        1. If absolute path exists: use it
        2. If relative, try cli/rag/collections/{path}
        3. If name only (no .yaml), try cli/rag/collections/{name}.yaml

        Args:
            yaml_path: Path to resolve

        Returns:
            Resolved Path object
        """
        path = Path(yaml_path)

        # Absolute path
        if path.is_absolute():
            return path

        # Relative to cli/rag/collections/
        # Find project root (contains cli/rag/collections/)
        current = Path(__file__).resolve()
        while current.parent != current:
            collections_dir = current / "cli" / "rag" / "collections"
            if collections_dir.exists():
                # Try with current name
                candidate = collections_dir / path
                if candidate.exists():
                    return candidate

                # Try adding .yaml extension
                if not str(path).endswith(".yaml"):
                    candidate_yaml = collections_dir / f"{path}.yaml"
                    if candidate_yaml.exists():
                        return candidate_yaml

                # Return candidate path (may not exist, caller handles error)
                return collections_dir / path

            current = current.parent

        # Fallback: return as-is (likely won't exist)
        return path

    def _load_documents(self) -> None:
        """Load all matching documents from filesystem.

        Reuses pattern from ContextDocsWidget.
        """
        for pattern in self.whitelist_paths:
            if pattern.endswith("/"):
                # Directory pattern - get all files recursively
                search_path = self.base_path / pattern.rstrip("/")
                if search_path.exists() and search_path.is_dir():
                    for file_path in search_path.rglob("*"):
                        if file_path.is_file():
                            self._load_file_if_not_excluded(file_path)
            else:
                # Glob pattern
                for file_path in self.base_path.glob(pattern):
                    if file_path.is_file():
                        self._load_file_if_not_excluded(file_path)

    def _is_text_file(self, file_path: Path) -> bool:
        """Check if file is a text file.

        Reuses logic from ContextDocsWidget.
        """
        # Skip files > 1MB
        if file_path.exists() and file_path.stat().st_size > 1_000_000:
            return False

        # Common text file extensions
        text_extensions = {
            ".py",
            ".js",
            ".ts",
            ".jsx",
            ".tsx",
            ".java",
            ".c",
            ".cpp",
            ".h",
            ".cs",
            ".php",
            ".rb",
            ".go",
            ".rs",
            ".swift",
            ".kt",
            ".sh",
            ".html",
            ".css",
            ".xml",
            ".svg",
            ".json",
            ".yaml",
            ".yml",
            ".toml",
            ".ini",
            ".cfg",
            ".conf",
            ".env",
            ".md",
            ".rst",
            ".txt",
            ".dockerfile",
            ".makefile",
            ".sql",
        }

        # Check common filenames without extensions
        filename = file_path.name.lower()
        if filename in {"dockerfile", "makefile", "readme", "license", "changelog"}:
            return True

        # Check extension
        if file_path.suffix.lower() in text_extensions:
            return True

        # Fallback to mime type
        mime_type, _ = mimetypes.guess_type(str(file_path))
        return mime_type and mime_type.startswith("text/")

    def _load_file_if_not_excluded(self, file_path: Path) -> None:
        """Load file if it passes all filters."""
        try:
            rel_path = file_path.relative_to(self.base_path)
        except ValueError:
            return

        # Skip hidden files/directories
        if any(part.startswith(".") for part in rel_path.parts):
            return

        # Check gitignore
        rel_path_str = str(rel_path)
        if self._gitignore_spec and self._gitignore_spec.match_file(rel_path_str):
            return

        # Check blacklist
        for exclude_pattern in self.blacklist_paths:
            if fnmatch.fnmatch(rel_path_str, exclude_pattern):
                return

        # Check if text file
        if not self._is_text_file(file_path):
            return

        # Load the file
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            self.documents[rel_path_str] = content
            self.total_chars += len(content)

        except (UnicodeDecodeError, Exception) as e:
            # Skip files that can't be read
            print(f"âš ï¸ Could not load {rel_path_str}: {e}")

    def compute_file_hash(self, file_path: str) -> str:
        """Compute SHA256 hash of a single file's content.

        Args:
            file_path: Relative path to file

        Returns:
            SHA256 hex string (64 characters)
        """
        content = self.documents.get(file_path, "")
        return hashlib.sha256(content.encode("utf-8")).hexdigest()

    def _extract_query_from_prompt(self, user_message: str, max_chars: int = 1000) -> str:
        """Extract query from user prompt for ambient RAG search.

        If prompt is >1000 chars, use last 1000 chars.

        Args:
            user_message: User's input message
            max_chars: Maximum characters to use (default: 1000)

        Returns:
            Query string for vector search
        """
        if len(user_message) <= max_chars:
            return user_message

        # Use last 1000 characters
        return user_message[-max_chars:]

    async def _ensure_embedding_service(self) -> VoyageEmbeddingService:
        """Lazily create embedding service (needs async context)."""
        if self._embedding_service is None:
            self._embedding_service = VoyageEmbeddingService()
        return self._embedding_service

    async def _initialize_embeddings(self) -> None:
        """Generate and store embeddings using per-file hashing with global deduplication.

        Enhanced algorithm with cross-collection reuse:
        1. Check collection for existing file_hash (same collection reuse)
        2. Check global registry for (absolute_path, content_hash, chunk_params)
        3. If found in registry: copy chunks from source collection (cross-collection reuse)
        4. If not found: chunk, embed, store, and register

        This eliminates redundant embeddings across collections that reference same files.
        """
        if self._initialized:
            return

        if not self.documents:
            print("âš ï¸ No documents to embed")
            self._initialized = True
            return

        base_path_str = str(self.base_path.resolve())
        chunk_params = ChunkParams(self.chunk_min_tokens, self.chunk_max_tokens)

        # Get existing file hashes from current collection
        existing_file_hashes = self.vector_store.get_file_hashes(base_path_str)

        # Categorize files
        files_same_collection_reuse = []  # In current collection, hash matches
        files_cross_collection_reuse = []  # Found in global registry
        files_to_update = []  # In current collection, hash changed
        files_to_embed = []  # Not in collection or registry

        # Track global registry hits for reporting
        registry_hits = {}  # file_path â†’ RegistryEntry

        for file_path in self.documents.keys():
            current_hash = self.compute_file_hash(file_path)
            existing_hash = existing_file_hashes.get(file_path)

            # Check 1: Does current collection have this file with matching hash?
            if existing_hash == current_hash:
                files_same_collection_reuse.append(file_path)
                continue

            # Check 2: Does global registry have this file?
            absolute_path = str((self.base_path / file_path).resolve())
            registry_entry = self.global_registry.lookup(
                absolute_path=absolute_path, content_hash=current_hash, chunk_params=chunk_params
            )

            if registry_entry:
                # Found in global registry - can reuse chunks
                files_cross_collection_reuse.append(file_path)
                registry_hits[file_path] = registry_entry
            elif existing_hash is not None:
                # File exists in current collection but hash changed
                files_to_update.append(file_path)
            else:
                # File is new (not in collection or registry)
                files_to_embed.append(file_path)

        # Report status
        total_files = len(self.documents)
        if files_same_collection_reuse:
            print(
                f"âœ“ {len(files_same_collection_reuse)}/{total_files} files unchanged in collection (reusing embeddings)"
            )
        if files_cross_collection_reuse:
            print(
                f"â™»ï¸  {len(files_cross_collection_reuse)}/{total_files} files reused from other collections (global registry)"
            )
        if files_to_embed:
            print(f"ðŸ”„ {len(files_to_embed)}/{total_files} new files to embed")
        if files_to_update:
            print(f"ðŸ”„ {len(files_to_update)}/{total_files} changed files to re-embed")

        # Handle cross-collection reuse
        # Track successful copies for accurate metrics
        successful_registry_reuse = []

        for file_path in files_cross_collection_reuse:
            entry = registry_hits[file_path]
            current_hash = self.compute_file_hash(file_path)

            # Delete existing chunks if file exists in current collection
            if file_path in existing_file_hashes:
                deleted_count = self.vector_store.delete_file_chunks(base_path_str, file_path)
                print(f"  â†’ Deleted {deleted_count} old chunks for {file_path}")

            # Copy chunks from source collection
            try:
                copied_count = self.vector_store.copy_chunks(
                    source_collection_name=entry.collection_name,
                    source_chunk_ids=entry.chunk_ids,
                    target_base_path=base_path_str,
                    target_file_path=file_path,
                    target_file_hash=current_hash,
                )
                print(
                    f"  â™»ï¸  Copied {copied_count} chunks for {file_path} from {entry.collection_name}"
                )
                successful_registry_reuse.append(file_path)
            except ValueError as e:
                # If copy fails (source not found, chunks missing), fall back to embedding
                print(f"  âš ï¸ Failed to copy chunks for {file_path} (ValueError: {e})")
                print("  â†’ Will embed this file instead")
                files_to_embed.append(file_path)
            except Exception as e:
                # Unexpected error - log but re-raise
                print(
                    f"  âš ï¸ Unexpected error copying chunks for {file_path} ({type(e).__name__}: {e})"
                )
                raise

        # Delete old chunks for updated files
        for file_path in files_to_update:
            deleted_count = self.vector_store.delete_file_chunks(base_path_str, file_path)
            print(f"  â†’ Deleted {deleted_count} old chunks for {file_path}")

        # Process files that need embedding
        files_needing_work = files_to_embed + files_to_update

        if not files_needing_work:
            # All files reused (same or cross-collection)
            self.active_file_paths = list(self.documents.keys())
            self._initialized = True
            return

        # Chunk and embed files
        embedding_service = await self._ensure_embedding_service()

        # Collect all chunks across files for batch processing
        all_file_chunks = []  # List of (file_path, file_hash, chunk_texts, chunk_metadatas)

        for file_path in files_needing_work:
            content = self.documents[file_path]
            file_hash = self.compute_file_hash(file_path)

            chunks = self.chunker.chunk_document(content, file_path)

            # Skip files with no chunks (too small or no content)
            if not chunks:
                print(f"  â†’ {file_path}: 0 chunks (skipping - file too small)")
                continue

            chunk_texts = []
            chunk_metadatas = []

            for chunk in chunks:
                chunk_texts.append(chunk.content)
                chunk_metadatas.append(
                    {
                        "file_path": file_path,
                        "file_hash": file_hash,
                        "chunk_index": chunk.chunk_index,
                        "token_count": chunk.token_count,
                        "start_char": chunk.start_char,
                        "end_char": chunk.end_char,
                    }
                )

            all_file_chunks.append((file_path, file_hash, chunk_texts, chunk_metadatas))
            print(f"  â†’ {file_path}: {len(chunks)} chunks")

        # If no files have chunks, we're done
        if not all_file_chunks:
            print("  âš ï¸ No files generated chunks (all files too small)")
            self.active_file_paths = list(self.documents.keys())
            self._initialized = True
            return

        # Flatten for batch embedding
        all_chunks = []
        chunk_to_file_map = []  # Track which file each chunk belongs to

        for file_idx, (file_path, file_hash, chunk_texts, chunk_metadatas) in enumerate(
            all_file_chunks
        ):
            for chunk_text in chunk_texts:
                all_chunks.append(chunk_text)
                chunk_to_file_map.append(file_idx)

        print(f"  â†’ Total: {len(all_chunks)} chunks across {len(all_file_chunks)} files to embed")

        # Batch process embeddings with token-aware batching
        max_batch_items = 128
        max_batch_tokens = 60_000
        all_embeddings = []

        current_batch = []
        current_batch_tokens = 0
        batch_num = 0

        for i, chunk in enumerate(all_chunks):
            file_idx = chunk_to_file_map[i]
            _, _, _, metadatas = all_file_chunks[file_idx]
            # Find the chunk metadata for this chunk within its file
            chunk_idx_in_file = sum(1 for j in range(i) if chunk_to_file_map[j] == file_idx)
            chunk_tokens = metadatas[chunk_idx_in_file]["token_count"]

            would_exceed_items = len(current_batch) >= max_batch_items
            would_exceed_tokens = current_batch_tokens + chunk_tokens > max_batch_tokens

            if current_batch and (would_exceed_items or would_exceed_tokens):
                batch_num += 1
                print(
                    f"  â†’ Batch {batch_num}: {len(current_batch)} chunks ({current_batch_tokens:,} tokens)..."
                )
                result = await embedding_service.embed_texts(current_batch, input_type="document")
                all_embeddings.extend(result.embeddings)
                print(f"  âœ“ Batch {batch_num} complete")

                import asyncio

                await asyncio.sleep(3)

                current_batch = [chunk]
                current_batch_tokens = chunk_tokens
            else:
                current_batch.append(chunk)
                current_batch_tokens += chunk_tokens

        # Final batch
        if current_batch:
            batch_num += 1
            print(
                f"  â†’ Final batch {batch_num}: {len(current_batch)} chunks ({current_batch_tokens:,} tokens)..."
            )
            result = await embedding_service.embed_texts(current_batch, input_type="document")
            all_embeddings.extend(result.embeddings)
            print(f"  âœ“ Final batch {batch_num} complete")

        # Get current collection name for registry
        collection_name = self.vector_store.get_collection_name(base_path_str)

        # Store embeddings per file and register in global registry
        embedding_idx = 0
        for file_path, file_hash, chunk_texts, chunk_metadatas in all_file_chunks:
            file_embeddings = all_embeddings[embedding_idx : embedding_idx + len(chunk_texts)]

            # Store in ChromaDB
            self.vector_store.store_file_chunks(
                base_path=base_path_str,
                file_path=file_path,
                file_hash=file_hash,
                chunks=chunk_texts,
                embeddings=file_embeddings,
                metadatas=chunk_metadatas,
            )

            # Generate chunk IDs (must match what store_file_chunks creates)
            import hashlib

            file_id_prefix = hashlib.sha256(file_path.encode("utf-8")).hexdigest()[:8]
            chunk_ids = [f"{file_id_prefix}_{i}" for i in range(len(chunk_texts))]

            # Register in global registry
            absolute_path = str((self.base_path / file_path).resolve())
            self.global_registry.register(
                absolute_path=absolute_path,
                content_hash=file_hash,
                chunk_params=chunk_params,
                collection_name=collection_name,
                chunk_ids=chunk_ids,
            )

            embedding_idx += len(chunk_texts)
            print(f"  âœ“ Stored {len(chunk_texts)} chunks for {file_path}")

        # Set active file paths for query filtering
        self.active_file_paths = list(self.documents.keys())
        self._initialized = True

        # Final summary - compute actual reuse metrics
        same_collection_reuse_count = len(files_same_collection_reuse)
        registry_reuse_count = len(successful_registry_reuse)
        reused_chunks_from_registry = sum(
            len(registry_hits[fp].chunk_ids) for fp in successful_registry_reuse
        )

        print(f"âœ“ RAG embeddings ready ({len(self.documents)} files)")
        if registry_reuse_count > 0:
            print(
                f"  â†’ Reused {reused_chunks_from_registry} chunks from global registry (saved API calls!)"
            )
        if same_collection_reuse_count > 0:
            print(f"  â†’ {same_collection_reuse_count} files unchanged in this collection")

    async def on_user_input(self, message: str, ctx: "StepContext") -> None:
        """Initialize embeddings on first user input (lazy loading).

        This hook is called when user sends a message. We use it to
        ensure embeddings are generated before the agent runs.

        Per-file hashing means:
        - Unchanged files reuse existing embeddings (fast)
        - Changed files trigger re-embedding only for those files
        - New files are embedded and added to the collection
        - Cross-widget reuse within same base_path

        In ambient_only mode, also performs vector search and stores results
        for injection into ambient context.

        Args:
            message: User's message (used for ambient RAG search)
            ctx: Step context (not used, but required by hook signature)
        """
        # Initialize embeddings if not already done
        await self._initialize_embeddings()

        # In ambient mode, perform search on user input
        if self.ambient_only:
            await self._perform_ambient_search(message)

    async def _perform_ambient_search(self, user_message: str) -> None:
        """Perform vector search for ambient mode and store results.

        Args:
            user_message: User's input message to search against
        """
        if not self.active_file_paths:
            self._ambient_results = None
            return

        # Extract query from user prompt
        query = self._extract_query_from_prompt(user_message)

        # Generate query embedding
        embedding_service = await self._ensure_embedding_service()
        query_embedding = await embedding_service.embed_single(query, input_type="query")

        # Search vector store with file filtering
        base_path_str = str(self.base_path.resolve())
        results = self.vector_store.search(
            base_path=base_path_str,
            query_embedding=query_embedding,
            file_paths=self.active_file_paths,
            top_k=self.top_k,
        )

        if not results:
            self._ambient_results = None
            return

        # Format results for ambient context
        output_lines = ["# Relevant Documentation (from RAG search)\n"]
        output_lines.append(f"Based on your query, here are {len(results)} relevant excerpts:\n")

        for i, result in enumerate(results, 1):
            file_path = result.metadata.get("file_path", "unknown")
            chunk_idx = result.metadata.get("chunk_index", 0)
            score = result.score

            # Determine chunk range display
            if self.include_neighboring_chunks:
                start_idx = max(0, chunk_idx - 1)
                end_idx = chunk_idx + 1
                chunk_range = f"chunks {start_idx}-{end_idx}"
            else:
                chunk_range = f"chunk {chunk_idx}"

            output_lines.append(f"## Excerpt {i} (relevance: {score:.2f})")
            output_lines.append(f"**Source:** {file_path} ({chunk_range})")
            output_lines.append("")

            # Expand with context if requested
            if self.include_neighboring_chunks:
                expanded_content = self._expand_result_with_context(result, base_path_str)
                output_lines.append(expanded_content)
            else:
                output_lines.append(result.content)

            output_lines.append("")  # Blank line between excerpts

        self._ambient_results = "\n".join(output_lines)

    def _expand_result_with_context(self, result, base_path_str: str) -> str:
        """Expand a search result with neighboring chunks for context.

        Args:
            result: SearchResult object with content and metadata
            base_path_str: Absolute path to base directory

        Returns:
            Formatted string with context chunks and delimiters
        """
        file_path = result.metadata.get("file_path", "unknown")
        chunk_idx = result.metadata.get("chunk_index", 0)

        # Calculate neighbor indices
        neighbor_indices = []
        if chunk_idx > 0:
            neighbor_indices.append(chunk_idx - 1)
        neighbor_indices.append(chunk_idx)
        if chunk_idx >= 0:  # Always try to get next chunk
            neighbor_indices.append(chunk_idx + 1)

        # Retrieve neighboring chunks
        chunk_map = self.vector_store.get_chunks_by_indices(
            base_path=base_path_str, file_path=file_path, chunk_indices=neighbor_indices
        )

        # Build output with delimiters
        output_parts = []

        # Previous chunk (if exists)
        if chunk_idx - 1 in chunk_map:
            output_parts.append(f"[Context: chunk {chunk_idx - 1}]")
            output_parts.append(chunk_map[chunk_idx - 1])
            output_parts.append("")  # Blank line

        # Matched chunk
        if chunk_idx in chunk_map:
            output_parts.append(f">> [Matched: chunk {chunk_idx}] <<")
            output_parts.append(chunk_map[chunk_idx])
            output_parts.append("")  # Blank line

        # Next chunk (if exists)
        if chunk_idx + 1 in chunk_map:
            output_parts.append(f"[Context: chunk {chunk_idx + 1}]")
            output_parts.append(chunk_map[chunk_idx + 1])

        return "\n".join(output_parts)

    def get_toolset(self, ctx: "StepContext") -> Optional[FunctionToolset]:
        """Provide search_documents tool for the agent.

        In ambient_only mode, returns None (no tool provided).

        Args:
            ctx: Step context (not used, but required by BasePlugin signature)

        Returns:
            FunctionToolset with RAG search tool, or None if ambient_only=True
        """
        # In ambient mode, don't provide tool
        if self.ambient_only:
            return None

        toolset = FunctionToolset()

        @toolset.tool
        async def search_documents(query: str) -> str:
            """Search documents using semantic similarity.

            Use this tool to find relevant information from the project's
            documentation and code. Provide a natural language query describing
            what you're looking for.

            Args:
                query: Search query (natural language)

            Returns:
                Formatted search results with relevant document excerpts
            """
            # Ensure embeddings are initialized
            await self._initialize_embeddings()

            if not self.active_file_paths:
                return "Error: No documents loaded for search."

            # Generate query embedding
            embedding_service = await self._ensure_embedding_service()
            query_embedding = await embedding_service.embed_single(query, input_type="query")

            # Search vector store with file filtering
            base_path_str = str(self.base_path.resolve())
            results = self.vector_store.search(
                base_path=base_path_str,
                query_embedding=query_embedding,
                file_paths=self.active_file_paths,
                top_k=self.top_k,
            )

            if not results:
                return "No relevant documents found for your query."

            # Format results
            output_lines = [f"Found {len(results)} relevant excerpts:\n"]

            for i, result in enumerate(results, 1):
                file_path = result.metadata.get("file_path", "unknown")
                chunk_idx = result.metadata.get("chunk_index", 0)
                score = result.score

                # Determine chunk range display
                if self.include_neighboring_chunks:
                    # Calculate actual range (accounting for edge cases)
                    start_idx = max(0, chunk_idx - 1)
                    end_idx = chunk_idx + 1
                    chunk_range = f"chunks {start_idx}-{end_idx}"
                else:
                    chunk_range = f"chunk {chunk_idx}"

                output_lines.append(f"## Result {i} (relevance: {score:.2f})")
                output_lines.append(f"**File:** {file_path} ({chunk_range})")
                output_lines.append("")

                # Expand with context if requested
                if self.include_neighboring_chunks:
                    expanded_content = self._expand_result_with_context(result, base_path_str)
                    output_lines.append(expanded_content)
                else:
                    output_lines.append(result.content)

                output_lines.append("")  # Blank line between results

            return "\n".join(output_lines)

        return toolset

    async def get_instructions(self, ctx: "StepContext") -> str | None:
        """Provide instructions about RAG capability.

        Args:
            ctx: Step context with state and deps

        In tool mode: Returns instructions for using the search_documents tool
        In ambient mode: Returns the search results directly (ephemeral, per-turn)

        Returns:
            Instructions or search results, depending on mode
        """
        if not self.documents:
            return None

        # In ambient mode, return search results (if any)
        if self.ambient_only:
            return self._ambient_results

        # In tool mode, return instructions
        return f"""
# RAG Document Search

You have access to semantic search over {len(self.documents)} documents
({self.total_chars:,} characters) from the project.

Use the `search_documents` tool to find relevant information. The search uses
embeddings for semantic similarity, so you can use natural language queries.

Example: search_documents("How does authentication work?")
"""

    # ========================================================================
    # BlueprintProtocol Serialization
    # ========================================================================

    def _serialize_config(self) -> dict:
        """Serialize widget configuration to dict.

        Returns:
            Config dict with RAG settings and paths
        """
        return {
            "base_path": str(self.base_path),
            "whitelist_paths": self.whitelist_paths.copy(),
            "blacklist_paths": self.blacklist_paths.copy(),
            "chunk_min_tokens": self.chunk_min_tokens,
            "chunk_max_tokens": self.chunk_max_tokens,
            "top_k": self.top_k,
            "ambient_only": self.ambient_only,
            "include_neighboring_chunks": self.include_neighboring_chunks,
        }

    # Note: to_blueprint_config() inherited from BasePlugin

    @classmethod
    def from_blueprint_config(cls, config: "ComponentConfig", agent: "Agent") -> "RAGWidget":
        """Deserialize from BlueprintProtocol format.

        Args:
            config: ComponentConfig from Blueprint
            agent: Agent instance that owns this widget
        """
        widget = cls(
            base_path=config.config["base_path"],
            whitelist_paths=config.config["whitelist_paths"],
            blacklist_paths=config.config.get("blacklist_paths", []),
            chunk_min_tokens=config.config.get("chunk_min_tokens", 400),
            chunk_max_tokens=config.config.get("chunk_max_tokens", 600),
            top_k=config.config.get("top_k", 5),
            ambient_only=config.config.get("ambient_only", False),
            include_neighboring_chunks=config.config.get("include_neighboring_chunks", False),
        )
        widget.instance_id = config.instance_id
        return widget
